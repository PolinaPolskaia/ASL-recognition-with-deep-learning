{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56a3d6a2-3f57-416f-8341-d9d3fc3c4f26",
   "metadata": {},
   "source": [
    "# ASL Recognition with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d816c-db25-4831-bf33-af76943392a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Project Overview](#chapter1)\n",
    "* [Loading the ASL Image Dataset](#chapter2)\n",
    "* [Examining the Dataset](#chapter3)\n",
    "* [One-hot encoding the data](#chapter4)\n",
    "* [Defining the model](#chapter5)\n",
    "* [Compiling the model](#chapter6)\n",
    "* [Training the model](#chapter7)\n",
    "* [Testing the model](#chapter8)\n",
    "* [Visualizing mistakes](#chapter9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2483e3c-50d2-46ea-a525-b74fa7ef7263",
   "metadata": {},
   "source": [
    "## 1. Project Overview <a class=\"anchor\" id=\"chapter1\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f254c7-6c16-42cc-9535-e4b6781a15c5",
   "metadata": {},
   "source": [
    "American Sign Language (ASL) is the primary language used by many deaf individuals in North America, and it is also used by hard-of-hearing and hearing individuals. The language is as rich as spoken languages and employs signs made with the hand, along with facial gestures and bodily postures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff2b843-6058-4244-9113-3caeadee99b2",
   "metadata": {},
   "source": [
    "![](pictures/asl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2bc4b-88ce-40b7-b503-3f9c94e18c7f",
   "metadata": {},
   "source": [
    "A lot of recent progress has been made towards developing computer vision systems that translate sign language to spoken language. This technology often relies on complex neural network architectures that can detect subtle patterns in streaming video. However, as a first step, towards understanding how to build a translation system, we can reduce the size of the problem by translating individual letters, instead of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8e8a7-451b-4b11-bb6c-fb52bd3a5dd6",
   "metadata": {},
   "source": [
    "**In this notebook**, we will train a convolutional neural network to classify images of American Sign Language (ASL) letters. After loading, examining, and preprocessing the data, we will train the network and test its performance.\n",
    "\n",
    "In the code cell below, we load the training and test data.\n",
    "\n",
    "* `x_train` and `x_test` are arrays of image data.\n",
    "* `y_train` and `y_test` are arrays of category labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e5087-9592-49e2-8298-3ed9041b414b",
   "metadata": {},
   "source": [
    "## 2. Loading the ASL Image Dataset <a class=\"anchor\" id=\"chapter2\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acebae7-f5be-4b82-9c24-aa64d1a023f2",
   "metadata": {},
   "source": [
    "To load the training data, we will:\n",
    "* Define the file paths for the training and testing datasets\n",
    "* Create a list of folders' names for each ASL symbol to avoid using `os.listdir()`\n",
    "* Load the needed libraries and set NumPy random seed\n",
    "* Define functions to load, shuffle, and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c510aa25-6197-4403-b2b5-78c98ca10831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file paths for the training and testing datasets\n",
    "train_dir = '/Users/inmilk306/Documents/projects/ASL_Recognition w_DL/archive/asl_alphabet_train/asl_alphabet_train'\n",
    "test_dir = '/Users/inmilk306/Documents/projects/ASL_Recognition w_DL/archive/asl_alphabet_test/asl_alphabet_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7233a898-a462-489a-a692-c9b8770f1f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of folders' names for each ASL symbol\n",
    "folder = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','del','nothing','space']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0019bb12-487a-451e-85c8-90c35559dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and set numpy random seed\n",
    "import random\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import skimage\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "\n",
    "np.random.seed(5) \n",
    "tf.random.set_seed(2)\n",
    "%matplotlib inline\n",
    "\n",
    "# Define functions to load, shuffle, and split the data\n",
    "def load_data(container_path=train_dir, folders=folder,\n",
    "              size=87000, test_split=0.2, seed=0): # 3000 images in 29 classes\n",
    "    \"\"\"\n",
    "    Loads sign language dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames, labels = [], []\n",
    "\n",
    "    for label, folder in enumerate(folders):\n",
    "        folder_path = join(container_path, folder)\n",
    "        images = [join(folder_path, d)\n",
    "                     for d in sorted(listdir(folder_path))]\n",
    "        labels.extend(len(images) * [label])\n",
    "        filenames.extend(images)\n",
    "    \n",
    "    random.seed(seed)\n",
    "    data = list(zip(filenames, labels))\n",
    "    random.shuffle(data)\n",
    "    data = data[:size]\n",
    "    filenames, labels = zip(*data)\n",
    "\n",
    "    \n",
    "    # Get the images\n",
    "    x = paths_to_tensor(filenames).astype('float32')/255\n",
    "    # Store the one-hot targets\n",
    "    y = np.array(labels)\n",
    "\n",
    "    x_train = np.array(x[:int(len(x) * (1 - test_split))])\n",
    "    y_train = np.array(y[:int(len(x) * (1 - test_split))])\n",
    "    x_test = np.array(x[int(len(x) * (1 - test_split)):])\n",
    "    y_test = np.array(y[int(len(x) * (1 - test_split)):])\n",
    "     \n",
    "    print('Loaded', len(x_train),'images for training,','Train data shape =',x_train.shape)\n",
    "    print('Loaded', len(x_test),'images for testing','Test data shape =',x_test.shape)\n",
    "    print('Loaded', len(x_train),'labels for training,','Train data shape =',y_train.shape)\n",
    "    print('Loaded', len(x_test),'labels for testing','Test data shape =',y_test.shape)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path, size):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(size, size))\n",
    "    # convert PIL.Image.Image type to 3D tensor\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor \n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths, size=50):\n",
    "    list_of_tensors = [path_to_tensor(img_path, size) for img_path in img_paths]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effeb4f9-1496-4d3b-a097-18c495019075",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/c_/pb_69mrs7kb5bq1mhxkjtg340000gn/T/ipykernel_21951/2516074316.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load pre-shuffled training and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/c_/pb_69mrs7kb5bq1mhxkjtg340000gn/T/ipykernel_21951/2385590533.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(container_path, folders, size, test_split, seed)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Get the images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Store the one-hot targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c_/pb_69mrs7kb5bq1mhxkjtg340000gn/T/ipykernel_21951/2385590533.py\u001b[0m in \u001b[0;36mpaths_to_tensor\u001b[0;34m(img_paths, size)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mlist_of_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c_/pb_69mrs7kb5bq1mhxkjtg340000gn/T/ipykernel_21951/2385590533.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mlist_of_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/c_/pb_69mrs7kb5bq1mhxkjtg340000gn/T/ipykernel_21951/2385590533.py\u001b[0m in \u001b[0;36mpath_to_tensor\u001b[0;34m(img_path, size)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpath_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# loads RGB image as PIL.Image.Image type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# convert PIL.Image.Image type to 3D tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    311\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[0;32m--> 313\u001b[0;31m   return image.load_img(path, grayscale=grayscale, color_mode=color_mode,\n\u001b[0m\u001b[1;32m    314\u001b[0m                         target_size=target_size, interpolation=interpolation)\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras_preprocessing/image/utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'grayscale'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3016\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36m_open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3000\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3001\u001b[0m                     \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3002\u001b[0;31m                     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3003\u001b[0m                     \u001b[0m_decompression_bomb_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3004\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;31m# Factory for making JPEG and MPO instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjpeg_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJpegImageFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m         \u001b[0mmpheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             except (\n\u001b[1;32m    123\u001b[0m                 \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# end of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMARKER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                     \u001b[0mhandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFDA\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# start of scan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36mDQT\u001b[0;34m(self, marker)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"little\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the values are always big-endian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzigzag_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqt_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/JpegImagePlugin.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteorder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"little\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyteswap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# the values are always big-endian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzigzag_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqt_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load pre-shuffled training and test datasets\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d778e613-4daf-43b9-abb7-e3471afb12ac",
   "metadata": {},
   "source": [
    "## 3. Examining the Dataset <a class=\"anchor\" id=\"chapter3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6e4ca-dc0d-4719-b037-a1f1c846071d",
   "metadata": {},
   "source": [
    "Now we'll begin by creating a list of string-valued labels containing the letters that appear in the dataset. Then, we visualize the first several images in the training data, along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572115c3-6206-42d5-a5ac-38f80a6c0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first several training images, along with the labels\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(3, 12, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(x_train[i]))\n",
    "    ax.set_title(\"{}\".format(folder[y_train[i]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7da8d5-a6dd-46db-abb5-15fb9a74c361",
   "metadata": {},
   "source": [
    "Let's examine how many images of each letter can be found in the dataset.\n",
    "\n",
    "Remember that dataset has already been split into training and test sets, where `x_train` and `x_test` contain the images, and `y_train` and `y_test` contain their corresponding labels.\n",
    "\n",
    "For example, each entry in `y_train` and `y_test` is one of **0**, **1**, or **2**, corresponding to the letters 'A', 'B', and 'C', respectively.\n",
    "\n",
    "We will use the arrays `y_train` and `y_test` to verify that both the training and test sets each have roughly equal proportions of each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e730f-9e04-4362-8663-41a54a9709c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25,'space':26,'del':27,'nothing':28}\n",
    "\n",
    "# Number of each symbol in the training and testing datasets\n",
    "train_dict = {}\n",
    "test_dict = {}\n",
    "for key in labels_dict:\n",
    "    train_dict[key] = sum(y_train==labels_dict[key])\n",
    "    test_dict[key] = sum(y_test==labels_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2af858-6287-40a7-8fbb-61a1396da4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891959b-9008-4b5c-b92d-1767990c8483",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d3ba1a-02c6-4fec-ae8b-185ddfdf6eca",
   "metadata": {},
   "source": [
    "We have roughly equal proportions of each symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87c50a-b7ca-4b52-bf15-1e66251d71db",
   "metadata": {},
   "source": [
    "## 4. One-hot encoding the data <a class=\"anchor\" id=\"chapter4\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ba7613-8711-4dbd-bbe6-041c2f65a7e8",
   "metadata": {},
   "source": [
    "Currently, our labels for each of the letters are encoded as categorical integers, where, for example, 'A', 'B' and 'C' are encoded as 0, 1, and 2, respectively. However, Keras models do not accept labels in this format, and we must first one-hot encode the labels before supplying them to a Keras model.\n",
    "\n",
    "This conversion will turn the one-dimensional array of labels into a two-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047f060-1263-414e-aba6-d465c50c475b",
   "metadata": {},
   "source": [
    "![](pictures/onehot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cc7eb3-f29d-4ec0-ab4a-25bc524b2989",
   "metadata": {},
   "source": [
    "Each row in the two-dimensional array of one-hot encoded labels corresponds to a different image. The row has a 1 in the column that corresponds to the correct label, and 0 elsewhere.\n",
    "\n",
    "For instance,\n",
    "\n",
    "* 0 is encoded as [1, 0, 0],\n",
    "* 1 is encoded as [0, 1, 0], and\n",
    "* 2 is encoded as [0, 0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63338c7d-6e29-4fb5-bd1d-c7fa9284dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the training labels\n",
    "y_train_OH = to_categorical(y_train)\n",
    "\n",
    "# One-hot encode the test labels\n",
    "y_test_OH = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b78ef2-5ce5-47f3-9e19-75d5afa77956",
   "metadata": {},
   "source": [
    "## 5. Defining the model <a class=\"anchor\" id=\"chapter5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cebb26-ae81-47c5-b34f-89a685299132",
   "metadata": {},
   "source": [
    "Now it's time to define a convolutional neural network to classify the data.\n",
    "\n",
    "This network accepts an image of an American Sign Language letter as input. The output layer returns the network's predicted probabilities that the image belongs in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef59c6-ccb2-42dc-98a1-177fbf8a0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# First convolutional layer accepts image input\n",
    "model.add(Conv2D(filters=5, kernel_size=5, padding='same', activation='relu', \n",
    "                        input_shape=(50, 50, 3)))\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(filters=15, kernel_size=5, padding='same', activation='relu'))\n",
    "# Add another max pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(29, activation='softmax'))\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8827de86-31a9-4f98-b0ba-a8f013c7aa24",
   "metadata": {},
   "source": [
    "In the above waring, Tensorflow simply tells us that the version we have installed can use the AVX and AVX2 operations and is set to do so by default in certain situations (say inside a forward or back-prop matrix multiply), which can speed things up. This is not an error, it is just telling us that it can and will take advantage of our CPU to get that extra speed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd161c9e-cb41-4f0a-8172-6db84bbac92b",
   "metadata": {},
   "source": [
    "## 6. Compiling the model <a class=\"anchor\" id=\"chapter6\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a95e7-ff4b-426c-bb68-678a01aa68a7",
   "metadata": {},
   "source": [
    "After we have defined a neural network in Keras, the next step is to compile it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c102263-be01-4a8a-ba3a-40b4d1e7de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89245c-d79b-4743-9e25-479ccc9a71e3",
   "metadata": {},
   "source": [
    "## 7. Training the model <a class=\"anchor\" id=\"chapter7\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff031c-9169-40e6-b480-ae1c3cb30a12",
   "metadata": {},
   "source": [
    "Once we have compiled the model, we're ready to fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecafeda1-3027-4b10-88aa-1a8bf13ba931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "hist = model.fit(x_train,y_train_OH, epochs=50, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92589319-aec3-4f5f-86d6-e9610aac3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(hist.history['loss'], color='b', label=\"Train\")\n",
    "ax[0].plot(hist.history['val_loss'], color='r', label=\"validation\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(hist.history['accuracy'], color='b', label=\"Train\")\n",
    "ax[1].plot(hist.history['val_accuracy'], color='r',label=\"Validation\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef0263-ae09-471a-8d41-8ea3548f7580",
   "metadata": {},
   "source": [
    "## 8. Testing the model <a class=\"anchor\" id=\"chapter8\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb8d4e-295d-47ad-b861-def7bd2e5caf",
   "metadata": {},
   "source": [
    "To evaluate the model, we'll use the test dataset. This will tell us how the network performs when classifying images it has never seen before!\n",
    "\n",
    "If the classification accuracy on the test dataset is similar to the training dataset, this is a good sign that the model did not overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8278712c-6747-4c21-87cb-4c650597b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain accuracy on test set\n",
    "score = model.evaluate(x=x_test, \n",
    "                       y=y_test_OH,\n",
    "                       verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58cee6-c348-41e1-aff2-6ef7acc44e34",
   "metadata": {},
   "source": [
    "The classification accuracy on the test dataset is similar to the training dataset, so the model did not overfit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5d5786-f352-48ab-adc9-228e7772ffa8",
   "metadata": {},
   "source": [
    "## 9. Visualizing mistakes <a class=\"anchor\" id=\"chapter9\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0ef5d-8d9f-40bc-b3c4-e8ad65c37278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
